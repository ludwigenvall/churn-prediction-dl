{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKI8SETp7B5aAn5naFoSFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ludwigenvall/churn-prediction-dl/blob/main/Churn_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Churn Prediction with Simulated Customer Sequences using LSTM\n",
        "\n",
        "## Abstract\n",
        "\n",
        "Customer churn prediction is essential for subscription-based businesses. This project explores a Bayesian generative model used to simulate customer behavior over 30 days, followed by Long Short-Term Memory (LSTM) neural networks trained to classify churn. Results show high accuracy (93%) and strong performance metrics (F1-score: 0.86, AUC: 0.98), suggesting that combining generative simulation and deep learning is effective for churn modeling.\n"
      ],
      "metadata": {
        "id": "BujbHORBx314"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Churn, or the loss of customers, is costly for companies. Traditional churn models often rely on static features or simple heuristics, ignoring temporal dynamics. To capture customer behavior more realistically, this project uses two-steps:\n",
        "\n",
        "1. Simulate customer sequences with a Bayesian model based on churn status  \n",
        "2. Predict churn using sequential deep learning (LSTM)\n",
        "\n",
        "Question: *Can a deep learning model trained on synthetic, behavior-based sequences accurately detect churn?*\n"
      ],
      "metadata": {
        "id": "TDftdbbFxoAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "\n",
        "### Data Simulation\n",
        "\n",
        "Using the Telco Customer Churn dataset as a base for labels, customer behavior over 30 days was simulated using a PyMC Bayesian model. Three sequences were generated per customer:\n",
        "\n",
        "- `logins_seq`: daily number of logins (Poisson distribution)\n",
        "- `support_seq`: binary indicator of customer support contact (Binomial)\n",
        "- `data_seq`: daily data usage in GB (Gamma)\n",
        "\n",
        "The model samples from posterior distributions of parameters (e.g., churners have lower login rates and higher support contact probability). Random noise is added at generation to introduce variance across samples.\n",
        "\n",
        "A total of 7,035 customers were simulated. The churn label from the Telco dataset (Yes/No) was used as the target variable, while synthetic sequences were generated for each customer over 30 days. Data was split into train and test sets and shaped into `(n_samples, 30, 3)` tensors.\n",
        "\n"
      ],
      "metadata": {
        "id": "z1C4A5PKyC2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM is a type of recurrent neural network (RNN) designed to model sequential data and overcome the vanishing gradient problem associated with traditional RNNs. LSTMs achieve this using memory cells and gates (input, forget, and output) that control the flow of information across time steps.\n",
        "\n",
        "In this project, the LSTM architecture enables the model to capture temporal patterns in customer behavior over a 30-day window. The first LSTM layer extracts sequence-level features, while subsequent TimeDistributed dense layers apply learned transformations at each time step. A second LSTM layer then compresses the sequence into a fixed-size representation for the final churn prediction.\n",
        "\n",
        "LSTM is suitable for this problem due to its ability to remember past inputs over long periods and identify trends in customer actions that may indicate churn risk."
      ],
      "metadata": {
        "id": "S-aZAlS401LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Architecture\n",
        "\n",
        "Multiple LSTM models were implemented and evaluated:\n",
        "\n",
        "Baseline model: 1 LSTM layer -> Dropout -> Dense -> Output\n",
        "\n",
        "Deeper LSTM model: 2 stacked LSTM layers -> Dropout -> Dense layers\n",
        "\n",
        "Bidirectional LSTM: Bidirectional LSTM -> Dropout -> LSTM -> Dense\n",
        "\n",
        "Conv1D + LSTM: Conv1D + MaxPooling -> LSTM -> Dropout -> Dense\n",
        "\n",
        "TimeDistributed LSTM: LSTM -> TimeDistributed Dense -> LSTM -> Dropout -> Dense\n",
        "\n",
        "TimeDistributed with L2 regularization: Same as above + L2 kernel regularizer\n",
        "\n",
        "All models were trained using binary_crossentropy loss, Adam optimizer, early stopping with patience=5, batch size 64, and a validation split of 20%."
      ],
      "metadata": {
        "id": "oMzy-9xVyYeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Each model was evaluated using:\n",
        "\n",
        "- Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "- ROC Curve and AUC\n",
        "\n",
        "- Precision-Recall curve\n",
        "\n",
        "- Confusion Matrix"
      ],
      "metadata": {
        "id": "udCiunnx1P8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Among all tested architectures, the model with stacked TimeDistributed layers and LSTM achieved the best performance:\n",
        "\n",
        "- Accuracy: 93%\n",
        "\n",
        "- Precision (Churn): 0.91\n",
        "\n",
        "- Recall (Churn): 0.82\n",
        "\n",
        "- F1-score (Churn): 0.86\n",
        "\n",
        "- AUC (ROC): 0.98\n",
        "\n",
        "The baseline and simpler LSTM models performed slightly worse in recall and F1. Bidirectional and Conv1D-enhanced models yielded similar but marginally lower AUC scores. The use of L2 regularization slightly improved generalization.\n",
        "\n",
        "ROC and PR curves show strong class separation and high model confidence across models.\n",
        "\n",
        "Loss curves showed stable convergence in all models. Early stopping was triggered between epochs 10â€“18, with validation loss flattening earlier than training loss. This pattern supports the use of early stopping to reduce overfitting risk."
      ],
      "metadata": {
        "id": "2rwUBY5I1X3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "\n",
        "The combination of simulation and LSTM modeling produces robust results. The best-performing model generalized well despite being trained on synthetic data.\n",
        "\n",
        "Key observations include:\n",
        "\n",
        "- TimeDistributed layers helped capture finer patterns across time\n",
        "\n",
        "- Adding convolutional layers or bidirectionality gave only minor gains\n",
        "\n",
        "- L2 regularization improved stability slightly\n",
        "\n",
        "However, limitations include:\n",
        "\n",
        "- lack of real behavioral data\n",
        "\n",
        "- dependence on simulation assumptions\n",
        "\n",
        "- tradeoff between precision and recall (especially for churn class)\n",
        "\n",
        "Future work could test attention-based models or adapt simulations to better reflect real-world customer heterogeneity."
      ],
      "metadata": {
        "id": "z7CzjYvM2WQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code and Reproducibility\n",
        "\n",
        "All code is available in the GitHub repo: [github.com/ludwigenvall/churn-prediction-dl](https://github.com/ludwigenvall/churn-prediction-dl)\n",
        "\n",
        "Key scripts:\n",
        "\n",
        "- `generative_model.py`: Bayesian simulation\n",
        "\n",
        "- `lstm_model.py`: model utilities\n",
        "\n",
        "Notebooks:\n",
        "\n",
        "- `01_explore_telco_data.ipynb`: Exploring and cleaning the Telco dataset\n",
        "\n",
        "- `02_generate_behavior.ipynb`: Generating the simulated behavior and visualizing results\n",
        "\n",
        "- `03_train_lstm_model.ipynb`: full pipeline and evaluation"
      ],
      "metadata": {
        "id": "JmgZhLf72b4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "PyMC documentation: https://www.pymc.io/\n",
        "\n",
        "TensorFlow/Keras API: https://www.tensorflow.org/api_docs\n",
        "\n",
        "Telco Churn dataset: https://www.kaggle.com/blastchar/telco-customer-churn"
      ],
      "metadata": {
        "id": "19HeZ7504MYo"
      }
    }
  ]
}